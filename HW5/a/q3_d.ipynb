{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART D of Q3 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\Desktop\\gm\\gm5\\CSDI\n",
      "Requirement already satisfied: torch in c:\\users\\ali\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ali\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\ali\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ali\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (4.66.4)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ali\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (6.0.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ali\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (3.8.4)\n",
      "Collecting wget (from -r requirements.txt (line 7))\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting linear_attention_transformer (from -r requirements.txt (line 8))\n",
      "  Downloading linear_attention_transformer-0.19.1-py3-none-any.whl.metadata (787 bytes)\n",
      "Requirement already satisfied: filelock in c:\\users\\ali\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ali\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\ali\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ali\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2021.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ali\\anaconda3\\lib\\site-packages (from tqdm->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (3.0.9)\n",
      "Collecting axial-positional-embedding (from linear_attention_transformer->-r requirements.txt (line 8))\n",
      "  Downloading axial_positional_embedding-0.3.10-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: einops in c:\\users\\ali\\anaconda3\\lib\\site-packages (from linear_attention_transformer->-r requirements.txt (line 8)) (0.8.0)\n",
      "Collecting linformer>=0.1.0 (from linear_attention_transformer->-r requirements.txt (line 8))\n",
      "  Downloading linformer-0.2.3-py3-none-any.whl.metadata (602 bytes)\n",
      "Collecting local-attention (from linear_attention_transformer->-r requirements.txt (line 8))\n",
      "  Downloading local_attention-1.11.1-py3-none-any.whl.metadata (907 bytes)\n",
      "Collecting product-key-memory>=0.1.5 (from linear_attention_transformer->-r requirements.txt (line 8))\n",
      "  Downloading product_key_memory-0.2.11-py3-none-any.whl.metadata (717 bytes)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\ali\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->-r requirements.txt (line 1)) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\ali\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->-r requirements.txt (line 1)) (2021.13.1)\n",
      "Collecting colt5-attention>=0.10.14 (from product-key-memory>=0.1.5->linear_attention_transformer->-r requirements.txt (line 8))\n",
      "  Downloading CoLT5_attention-0.11.1-py3-none-any.whl.metadata (737 bytes)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.3)\n",
      "Collecting hyper-connections>=0.1.8 (from local-attention->linear_attention_transformer->-r requirements.txt (line 8))\n",
      "  Downloading hyper_connections-0.1.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ali\\anaconda3\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Downloading linear_attention_transformer-0.19.1-py3-none-any.whl (12 kB)\n",
      "Downloading linformer-0.2.3-py3-none-any.whl (6.2 kB)\n",
      "Downloading product_key_memory-0.2.11-py3-none-any.whl (6.5 kB)\n",
      "Downloading axial_positional_embedding-0.3.10-py3-none-any.whl (6.7 kB)\n",
      "Downloading local_attention-1.11.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading CoLT5_attention-0.11.1-py3-none-any.whl (18 kB)\n",
      "Downloading hyper_connections-0.1.9-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9681 sha256=56cf07c642538298d01c271c80b58f6d0fd6ee469372389b6c3c708d7d38f3a7\n",
      "  Stored in directory: c:\\users\\ali\\appdata\\local\\pip\\cache\\wheels\\01\\46\\3b\\e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
      "Successfully built wget\n",
      "Installing collected packages: wget, linformer, hyper-connections, axial-positional-embedding, local-attention, colt5-attention, product-key-memory, linear_attention_transformer\n",
      "Successfully installed axial-positional-embedding-0.3.10 colt5-attention-0.11.1 hyper-connections-0.1.9 linear_attention_transformer-0.19.1 linformer-0.2.3 local-attention-1.11.1 product-key-memory-0.2.11 wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ermongroup/CSDI.git\n",
    "%cd CSDI\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\Desktop\\gm\\gm5\\CSDI\n"
     ]
    }
   ],
   "source": [
    "%cd gm5/CSDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\Desktop\\gm\\gm5\\CSDI\n"
     ]
    }
   ],
   "source": [
    "%cd CSDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CSDI_Physio(\n",
       "  (embed_layer): Embedding(35, 16)\n",
       "  (diffmodel): diff_CSDI(\n",
       "    (diffusion_embedding): DiffusionEmbedding(\n",
       "      (projection1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (projection2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (input_projection): Conv1d(2, 64, kernel_size=(1,), stride=(1,))\n",
       "    (output_projection1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "    (output_projection2): Conv1d(64, 1, kernel_size=(1,), stride=(1,))\n",
       "    (residual_layers): ModuleList(\n",
       "      (0-3): 4 x ResidualBlock(\n",
       "        (diffusion_projection): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (cond_projection): Conv1d(145, 128, kernel_size=(1,), stride=(1,))\n",
       "        (mid_projection): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "        (output_projection): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "        (time_layer): TransformerEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TransformerEncoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (feature_layer): TransformerEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): TransformerEncoderLayer(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "              )\n",
       "              (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout1): Dropout(p=0.1, inplace=False)\n",
       "              (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "from main_model import CSDI_PM25, CSDI_Physio, CSDI_Forecasting\n",
    "from diff_models import diff_CSDI\n",
    "\n",
    "import torch \n",
    "\n",
    "device = 'cuda'\n",
    "path = \"config/base.yaml\"\n",
    "target_dim = 35\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "model = CSDI_Physio(config, device).to(device)\n",
    "model.load_state_dict(torch.load(\"save/pretrained/model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I just load the article model but don't finetune it for another dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
