{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7s1A4MiYjfEm",
        "g5dPBdpJ1BzF",
        "KLWTh5BLBQ4v",
        "e_yuR9x81au2",
        "2RcRwVvdATf5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative Models PHW4: Normalizing Flows\n",
        "\n",
        "Name:\n",
        "\n",
        "Student ID:"
      ],
      "metadata": {
        "id": "7s1A4MiYjfEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "g5dPBdpJ1BzF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGgRR8AijeMB"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "import imageio.v2 as imageio\n",
        "from matplotlib import pyplot as plt\n",
        "from typing import Callable, Optional, Tuple, Union, List\n",
        "from IPython.display import Image, display\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_moons, make_circles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\"cuda\" if torch.cuda.is_available()\n",
        "          else \"mps\" if torch.backends.mps.is_available()\n",
        "          else \"cpu\")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2FeUELgmpi6",
        "outputId": "20a70540-d42b-423a-87e1-9c761fc18e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title visualization helpers\n",
        "\n",
        "def visualize(flow, epoch, output_dir='frames', device='cpu'):\n",
        "    # Create directory for saving frames\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Visualization of the learned density\n",
        "    with torch.no_grad():\n",
        "        # Create a grid over the range of the data\n",
        "        xline = torch.linspace(-2, 3, 300, device=device)\n",
        "        yline = torch.linspace(-1, 1.5, 300, device=device)\n",
        "        xgrid, ygrid = torch.meshgrid(xline, yline, indexing='ij')\n",
        "        xyinput = torch.cat([xgrid.reshape(-1, 1),\n",
        "                             ygrid.reshape(-1, 1)], dim=1).to(device)\n",
        "\n",
        "        # Compute the log probability for each point in the grid\n",
        "        log_prob_grid = flow.log_prob(xyinput)  # Compute log probs\n",
        "        zgrid = torch.exp(log_prob_grid).reshape(300, 300).cpu()  # Convert to probability\n",
        "\n",
        "        # Plot the density\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.contourf(\n",
        "            xgrid.cpu().numpy(),\n",
        "            ygrid.cpu().numpy(),\n",
        "            zgrid.numpy(),\n",
        "            levels=50,\n",
        "            cmap='viridis'\n",
        "        )\n",
        "        plt.title(f'Learned Density at Epoch {epoch}')\n",
        "        plt.xlabel(r'$x_1$')\n",
        "        plt.ylabel(r'$x_2$')\n",
        "        plt.colorbar(label='Density')\n",
        "\n",
        "        # Save frame as an image\n",
        "        filename = os.path.join(output_dir, f'frame_{epoch:03d}.png')\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "\n",
        "def create_gif(output_dir='frames', gif_name='density_evolution.gif', fps=10):\n",
        "    images = []\n",
        "    for frame in sorted(os.listdir(output_dir)):\n",
        "        if frame.endswith(\".png\"):\n",
        "            images.append(imageio.imread(os.path.join(output_dir, frame)))\n",
        "    imageio.mimsave(gif_name, images, fps=fps)\n",
        "    display(Image(filename=gif_name))\n",
        "    return gif_name"
      ],
      "metadata": {
        "cellView": "form",
        "id": "crHHQ5A24k_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset (5 points)"
      ],
      "metadata": {
        "id": "KLWTh5BLBQ4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create moons using the `make_moons` dataset from `sklearn.datasets`. You must use `TensorDataset` to create a `DataLoader` for this dataset."
      ],
      "metadata": {
        "id": "2_CAEFqeGN_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate dataset\n",
        "X, _ = make_moons(n_samples=1000, noise=0.1)\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "\n",
        "# DataLoader\n",
        "dataset = TensorDataset(X)\n",
        "data_loader = DataLoader(dataset, batch_size=256, shuffle=True)"
      ],
      "metadata": {
        "id": "56S4wveZBSnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformations (30 points)"
      ],
      "metadata": {
        "id": "e_yuR9x81au2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the challenges in creating flow models is creating invertible transformations. Also we would like it if computation of their inverse and/or the log determinant of their jacobian were simple."
      ],
      "metadata": {
        "id": "KToowxDN1iof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transform(nn.Module):\n",
        "    \"\"\"Base class for all transform objects.\"\"\"\n",
        "\n",
        "    def forward(self, inputs, context=None):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def inverse(self, inputs, context=None):\n",
        "        raise 'InverseNotAvailable'"
      ],
      "metadata": {
        "id": "s_AsmPAS1fY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we can create a random permutation transform."
      ],
      "metadata": {
        "id": "zoweg3aeHD3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PermutationTransform(Transform):\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        permutation = torch.randperm(num_features)\n",
        "        self.register_buffer('permutation', permutation)\n",
        "        self.register_buffer('inverse_permutation', torch.argsort(permutation))\n",
        "\n",
        "    def forward(self, inputs, context=None):\n",
        "        return inputs[:, self.permutation], 0  # Log-det is zero\n",
        "\n",
        "    def inverse(self, inputs, context=None):\n",
        "        return inputs[:, self.inverse_permutation]"
      ],
      "metadata": {
        "id": "KIafrsqP27eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you must:\n",
        "\n",
        "1. **Scaling and Translation Networks**:\n",
        "\t- Use `self.s_net_factory` and `self.t_net_factory` to initialize the scaling (`s`) and translation (`t`) networks in the `__init__` method.\n",
        "\n",
        "2. **Forward Pass**:\n",
        "\t-\tPass fixed_inputs through `self.scaling_net` and `self.translation_net` to compute `s` and `t`.\n",
        "\n",
        "\t-\tApply the transformations to `transformed_inputs`.\n",
        "\n",
        "  - Compute the log-determinant of the Jacobian.\n",
        "\n",
        "3. **Inverse Pass**:\n",
        "\t-\tSolve for `transformed_inputs` by reversing the scaling and translation.\n",
        "\n",
        "4. **Utility Method **`_initialize_network`:\n",
        "\t-\tUse `nn.Sequential`to create a feedforward neural network with the desired hidden dimensions and activation functions."
      ],
      "metadata": {
        "id": "UrFUNm32KeQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CouplingTransform(Transform):\n",
        "    \"\"\"\n",
        "    Implements a coupling layer for a normalizing flow model.\n",
        "    It splits the input features into two groups based on a binary mask.\n",
        "    One group remains unchanged while the other is transformed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        mask: Union[torch.Tensor, list, tuple],\n",
        "        hidden_dims: int,\n",
        "        s_net_factory: Optional[Callable[[int, int, int], nn.Module]] = None,\n",
        "        t_net_factory: Optional[Callable[[int, int, int], nn.Module]] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mask (Union[torch.Tensor, list, tuple]): Binary mask specifying which features to transform.\n",
        "                * If `mask[i] > 0`, `input[i]` will be transformed.\n",
        "                * If `mask[i] <= 0`, `input[i]` will remain unchanged.\n",
        "            hidden_dims (int): Number of hidden units for the coupling neural networks.\n",
        "            s_net_factory (Callable): Function to create a neural network for the scaling function.\n",
        "                Must take (input_dim, output_dim, hidden_dims) as arguments.\n",
        "            t_net_factory (Callable): Function to create a neural network for the translation function.\n",
        "                Must take (input_dim, output_dim, hidden_dims) as arguments.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Validate and register the binary mask\n",
        "        mask = torch.as_tensor(mask, dtype=torch.float32)\n",
        "        if mask.dim() != 1:\n",
        "            raise ValueError(\"Mask must be a 1-dimensional tensor.\")\n",
        "        if mask.numel() == 0:\n",
        "            raise ValueError(\"Mask cannot be empty.\")\n",
        "        self.register_buffer('mask', mask)\n",
        "\n",
        "        # Number of input features\n",
        "        self.num_features = mask.numel()\n",
        "\n",
        "        # Validate and store the network factories\n",
        "        if not callable(s_net_factory) or not callable(t_net_factory):\n",
        "            raise ValueError(\"s_net_factory and t_net_factory must be callable functions.\")\n",
        "        self.s_net_factory = s_net_factory\n",
        "        self.t_net_factory = t_net_factory\n",
        "\n",
        "        # Initialize the scaling (s) and translation (t) networks\n",
        "        # These will be defined using the factories in the TODO section\n",
        "\n",
        "        # TODO: Define the networks (see instructions below)\n",
        "\n",
        "    def _initialize_network(\n",
        "        self, input_dim: int, output_dim: int, hidden_dims: int\n",
        "    ) -> nn.Module:\n",
        "        \"\"\"\n",
        "        Utility method to create a neural network.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): Number of input features for the network.\n",
        "            output_dim (int): Number of output features for the network.\n",
        "            hidden_dims (int): Number of hidden units for the network.\n",
        "\n",
        "        Returns:\n",
        "            nn.Module: A PyTorch neural network.\n",
        "        \"\"\"\n",
        "        # TODO: Use this function to initialize scaling and translation networks.\n",
        "        pass\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, context: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass through the coupling transform.\n",
        "\n",
        "        Args:\n",
        "            inputs (torch.Tensor): Input tensor of shape (batch_size, num_features).\n",
        "            context (Optional[torch.Tensor]): Optional context for conditional transformations.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]:\n",
        "                - Transformed inputs of the same shape as the input.\n",
        "                - Log-determinant of the Jacobian of the transformation.\n",
        "        \"\"\"\n",
        "        # Validate input dimensions\n",
        "        if inputs.dim() != 2:\n",
        "            raise ValueError(\"Inputs must be a 2D tensor.\")\n",
        "        if inputs.shape[1] != self.num_features:\n",
        "            raise ValueError(f\"Expected {self.num_features} features, but got {inputs.shape[1]}.\")\n",
        "\n",
        "        # Split inputs into fixed and transformed groups based on the mask\n",
        "        fixed_inputs = inputs * (1 - self.mask)\n",
        "        transformed_inputs = inputs * self.mask\n",
        "\n",
        "        # TODO: Compute scaling (s) and translation (t) using the neural networks\n",
        "        # Use the fixed_inputs as input to the networks\n",
        "        # Scale transformed_inputs by exp(s) and shift by t\n",
        "        # Optionally: Use tanh or clamp to keep scaling factors bounded\n",
        "\n",
        "        z = ...  # Combine fixed_inputs and transformed_inputs to form the output\n",
        "        log_det_J = ...  # Compute the log-determinant of the Jacobian\n",
        "\n",
        "        return z, log_det_J\n",
        "\n",
        "    def inverse(self, inputs: torch.Tensor, context: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Inverse pass through the coupling transform.\n",
        "\n",
        "        Args:\n",
        "            inputs (torch.Tensor): Input tensor of shape (batch_size, num_features).\n",
        "            context (Optional[torch.Tensor]): Optional context for conditional transformations.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Reconstructed inputs of the same shape as the input.\n",
        "        \"\"\"\n",
        "        # Validate input dimensions\n",
        "        if inputs.dim() != 2:\n",
        "            raise ValueError(\"Inputs must be a 2D tensor.\")\n",
        "        if inputs.shape[1] != self.num_features:\n",
        "            raise ValueError(f\"Expected {self.num_features} features, but got {inputs.shape[1]}.\")\n",
        "\n",
        "        # TODO: Similar to the forward pass, but solve for transformed_inputs\n",
        "        # Reverse the scaling and translation operations to reconstruct the input\n",
        "        z = ...\n",
        "\n",
        "        return z"
      ],
      "metadata": {
        "id": "dsmg6rlN2nTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flows (30 points)"
      ],
      "metadata": {
        "id": "2RcRwVvdATf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You must:\n",
        "\n",
        "1.\tForward Pass:\n",
        "\t-\tInitialize `log_det_J` to 0.\n",
        "\t-\tIterate over `self.transforms` and apply each transform's forward method.\n",
        "\n",
        "2.\tInverse Pass:\n",
        "\t-\tIterate through `self.transforms` in reverse order and apply each transform's inverse method.\n",
        "  - Return the final reconstructed input tensor.\n",
        "\n",
        "3.\tLog-Probability:\n",
        "\t-\tUse the forward method to get `z` and `log_det_J`.\n",
        "\t- Compute `log_prob_z` using the base distribution's `log_prob` method.\n",
        "  - Return `log_prob_z + log_det_J`.\n",
        "\n",
        "4.\tSampling:\n",
        "\t-\tSample z from the base distribution using `self.base_distribution.sample((num_samples,))`.\n",
        "\t-\tMap `z` back to the data space using the inverse method.\n",
        "\t-\tReturn the generated samples.\n",
        "\n",
        "5.\tSampling and Log-Probability:\n",
        "\t-\tUse the sample method to generate samples.\n",
        "\t-\tUse the `log_prob` method to compute log-probabilities for the generated samples.\n",
        "\t-\tReturn both the samples and their log-probabilities."
      ],
      "metadata": {
        "id": "y3dXHGRTNAgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flow(nn.Module):\n",
        "    \"\"\"\n",
        "    Flow-based model that combines a series of invertible transformations\n",
        "    with a base distribution for density estimation and sampling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        transforms: List[nn.Module],\n",
        "        base_distribution: Optional[MultivariateNormal] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            transforms (List[nn.Module]): List of invertible transformations (e.g., coupling layers).\n",
        "            base_distribution (Optional[MultivariateNormal]): The base distribution.\n",
        "                If None, defaults to a standard multivariate Gaussian.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.transforms = nn.ModuleList(transforms)\n",
        "        self.num_features = self.transforms[0].num_features  # Ensure all transforms share the same input size\n",
        "\n",
        "        if base_distribution is None:\n",
        "            self.base_distribution = MultivariateNormal(\n",
        "                loc=torch.zeros(self.num_features),\n",
        "                covariance_matrix=torch.eye(self.num_features)\n",
        "            )\n",
        "        else:\n",
        "            self.base_distribution = base_distribution\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass through the flow model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, num_features).\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]:\n",
        "                - Transformed tensor (latent representation) of the same shape as input.\n",
        "                - Log-determinant of the Jacobian of the entire transformation.\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        # 1. Initialize `log_det_J = 0` to accumulate log-determinants of Jacobians.\n",
        "        # 2. Iterate through `self.transforms` in order and apply each transform's `forward` method.\n",
        "        # 3. Update `log_det_J` with the log-determinant of each transform's Jacobian.\n",
        "        # 4. Return the final transformed tensor and accumulated log-determinant.\n",
        "\n",
        "        z = ...\n",
        "        log_det_J = ...\n",
        "\n",
        "        return z, log_det_J\n",
        "\n",
        "    def inverse(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Inverse pass through the flow model.\n",
        "\n",
        "        Args:\n",
        "            z (torch.Tensor): Latent tensor of shape (batch_size, num_features).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Reconstructed input tensor of the same shape as input.\n",
        "        \"\"\"\n",
        "        # TODO: Implement the inverse pass\n",
        "        # 1. Iterate through `self.transforms` in reverse order and apply each transform's `inverse` method.\n",
        "        # 2. Pass `z` sequentially through the inverses of the transforms.\n",
        "        # 3. Return the final reconstructed input tensor.\n",
        "\n",
        "        x = ...\n",
        "\n",
        "        return x\n",
        "\n",
        "    def log_prob(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute the log-probability of the input under the flow model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, num_features).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Log-probabilities of the input under the model.\n",
        "        \"\"\"\n",
        "        # TODO: Implement log-probability computation\n",
        "        # 1. Apply the forward pass to get `z` (latent representation) and `log_det_J`.\n",
        "        # 2. Compute `log_prob_z` using `self.base_distribution.log_prob(z)`.\n",
        "        # 3. Return the sum of `log_prob_z` and `log_det_J`.\n",
        "\n",
        "        z, log_det_J = ...\n",
        "        log_prob_z = ...\n",
        "        log_prob = ...\n",
        "\n",
        "        return log_prob\n",
        "\n",
        "    def sample(self, num_samples: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generate samples from the flow model.\n",
        "\n",
        "        Args:\n",
        "            num_samples (int): Number of samples to generate.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Samples of shape (num_samples, num_features).\n",
        "        \"\"\"\n",
        "        # TODO: Implement sampling\n",
        "        # 1. Sample from the base distribution to get `z`.\n",
        "        # 2. Apply the inverse pass to map `z` back to the data space.\n",
        "        # 3. Return the generated samples.\n",
        "\n",
        "        z = ...\n",
        "        x = ...\n",
        "\n",
        "        return x\n",
        "\n",
        "    def sample_and_log_prob(self, num_samples: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Generate samples and their log-probabilities under the flow model.\n",
        "\n",
        "        Args:\n",
        "            num_samples (int): Number of samples to generate.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, torch.Tensor]:\n",
        "                - Samples of shape (num_samples, num_features).\n",
        "                - Log-probabilities of the samples under the model.\n",
        "        \"\"\"\n",
        "        # TODO: Implement sample and log-prob computation\n",
        "        # 1. Use `sample` method to generate samples.\n",
        "        # 2. Use `log_prob` method to compute the log-probabilities of the samples.\n",
        "        # 3. Return both the samples and their log-probabilities.\n",
        "\n",
        "        x = ...\n",
        "        log_prob = ...\n",
        "\n",
        "        return x, log_prob"
      ],
      "metadata": {
        "id": "JCUccdvu2tWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masks (10 points)"
      ],
      "metadata": {
        "id": "z0Ez7VvYAXsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you should:\n",
        "\n",
        "1.\tGenerate the Masks:\n",
        "\t-\tLoop through num_masks and alternate between masks starting with 0 or 1.\n",
        "\n",
        "2.\tOutput Requirements:\n",
        "\t-\tEnsure the output is a list of `torch.Tensor` objects.\n",
        "\t-\tEach tensor should be of shape `(num_features,)` and should alternate patterns as described."
      ],
      "metadata": {
        "id": "SocgoUkOOms_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_alternating_masks(num_features: int, num_masks: int) -> List[torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Creates a sequence of alternating binary masks for coupling layers in normalizing flows.\n",
        "\n",
        "    Args:\n",
        "        num_features (int): The number of features in the input.\n",
        "        num_masks (int): The number of masks to generate.\n",
        "\n",
        "    Returns:\n",
        "        List[torch.Tensor]: A list of binary masks. Each mask is a 1D tensor of size `num_features`.\n",
        "                            Alternates between masks starting with 0s and masks starting with 1s.\n",
        "    \"\"\"\n",
        "    masks = []\n",
        "    # TODO:\n",
        "    # 1. Loop over `range(num_masks)` to create the required number of masks.\n",
        "    # 2. For each mask index `i`:\n",
        "    #     - Alternate between starting with 0s (when `i % 2 == 0`) and starting with 1s (when `i % 2 == 1`).\n",
        "    #     - For example, if `num_features=6`, the masks might look like:\n",
        "    #       [0, 1, 0, 1, 0, 1], [1, 0, 1, 0, 1, 0], ...\n",
        "    # 3. Convert the mask into a `torch.Tensor` and append it to the `masks` list.\n",
        "    # 4. Ensure each mask is of shape `(num_features,)`.\n",
        "\n",
        "    return masks"
      ],
      "metadata": {
        "id": "F0mFi4382vhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training (10 points)"
      ],
      "metadata": {
        "id": "pBLeZ6RRDZQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the model and it's hyperparameters."
      ],
      "metadata": {
        "id": "Uw3425I8Hk3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Create masks and flow\n",
        "num_features = ...\n",
        "num_masks = ...\n",
        "hidden_dims = ...\n",
        "masks = ...\n",
        "\n",
        "transforms = []\n",
        "for mask in masks:\n",
        "    # TODO\n",
        "    pass\n",
        "\n",
        "flow = ...\n",
        "optimizer = ..."
      ],
      "metadata": {
        "id": "K9BhkQI7DsMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now train the model. Make sure you run `visualize(flow, epoch)` at each epoch. (If you don't do this you won't be able to visualize the evolution of your model)"
      ],
      "metadata": {
        "id": "drI49wQsHjhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train the model\n"
      ],
      "metadata": {
        "id": "KWtx_o8v2yUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizations (15 points)"
      ],
      "metadata": {
        "id": "yE4j3Y3cApB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate some datapoints using the original distribution and from your model and plot them together."
      ],
      "metadata": {
        "id": "dzPnVTcKArrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Generate and visualize samples\n"
      ],
      "metadata": {
        "id": "WM5s5Llx_RT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the evolution of the Normalizing Flows model."
      ],
      "metadata": {
        "id": "D7O2BH9KJT-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gif_path = create_gif()"
      ],
      "metadata": {
        "id": "SR07gc88CeJY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}